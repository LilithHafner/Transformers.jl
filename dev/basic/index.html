<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Basic · Transformers.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="Transformers.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">Transformers.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li class="is-active"><a class="tocitem" href>Basic</a><ul class="internal"><li><a class="tocitem" href="#Transformer"><span>Transformer</span></a></li><li><a class="tocitem" href="#Positionwise"><span>Positionwise</span></a></li><li><a class="tocitem" href="#PositionEmbedding"><span>PositionEmbedding</span></a></li><li><a class="tocitem" href="#API-Reference"><span>API Reference</span></a></li></ul></li><li><a class="tocitem" href="../stacks/">Stacks</a></li><li><a class="tocitem" href="../pretrain/">Pretrain</a></li><li><span class="tocitem">Models</span><ul><li><a class="tocitem" href="../gpt/">GPT</a></li><li><a class="tocitem" href="../bert/">BERT</a></li></ul></li><li><a class="tocitem" href="../datasets/">Datasets</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Basic</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Basic</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/chengchingwen/Transformers.jl/blob/master/docs/src/basic.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Transformers.Basic"><a class="docs-heading-anchor" href="#Transformers.Basic">Transformers.Basic</a><a id="Transformers.Basic-1"></a><a class="docs-heading-anchor-permalink" href="#Transformers.Basic" title="Permalink"></a></h1><p>Basic functionality of Transformers.jl, provide the Transformer encoder/decoder implementation and other convenient function.</p><h2 id="Transformer"><a class="docs-heading-anchor" href="#Transformer">Transformer</a><a id="Transformer-1"></a><a class="docs-heading-anchor-permalink" href="#Transformer" title="Permalink"></a></h2><p>The <code>Transformer</code> and <code>TransformerDecoder</code> is the encoder and decoder block of the origin paper, and they are all implement as the  regular Flux Layer, so you can treat them just as the <code>Dense</code> layer. See the docstring for the argument. However, for the sequence  data input, we usually have a 3 dimensional input of shape <code>(hidden size, sequence length, batch size)</code> instead of just <code>(hidden size, batch size)</code>.  Therefore, we implement both 2d &amp; 3d operation according to the input type (The <code>N</code> of <code>Array{T, N}</code>). We are able to handle both input of shape  <code>(hidden size, sequence length, batch size)</code> and <code>(hidden size, sequence length)</code> for the case with only 1 input.</p><pre><code class="language-julia hljs">using Transformers

m = Transformer(512, 8, 64, 2048) #define a Transformer block with 8 head and 64 neuron for each head
x = randn(512, 30, 3) #fake data of length 30

y = m(x)</code></pre><h2 id="Positionwise"><a class="docs-heading-anchor" href="#Positionwise">Positionwise</a><a id="Positionwise-1"></a><a class="docs-heading-anchor-permalink" href="#Positionwise" title="Permalink"></a></h2><p>For the sequential task, we need to handle the 3 dimensional input. However, most of the layer in Flux only support input with shape  <code>(hidden size, batch size)</code>. In order to tackle this problem, we implement the <code>Positionwise</code> helper function that is almost the same  as <code>Flux.Chain</code> but it will run the model position-wisely. (internally it just reshape the input to 2d and apply the model then reshape  back). </p><pre><code class="language-julia hljs">using Transformers
using Flux

m = Positionwise(Dense(10, 5), Dense(5, 2), softmax)
x = randn(10, 30, 3)

y = m(x)

# which is equivalent to 
# 
# m = Chain(Dense(10, 5), Dense(5, 2), softmax)
# x1 = randn(10, 30)
# x2 = randn(10, 30)
# x3 = randn(10, 30)
# y = cat(m(x1), m(x2), m(x3); dims=3)</code></pre><h2 id="PositionEmbedding"><a class="docs-heading-anchor" href="#PositionEmbedding">PositionEmbedding</a><a id="PositionEmbedding-1"></a><a class="docs-heading-anchor-permalink" href="#PositionEmbedding" title="Permalink"></a></h2><p>We implement two kinds of position embedding, one is based on the sin/cos function (mentioned in the paper,  attention is all you need). Another one is just like regular word embedding but with the position index. The  first argument is the <code>size</code>. Since the position embedding is only related to the length of the input ( we use <code>size(input, 2)</code> as the length), the return value of the layer will be the embedding of the given  length without duplicate to the batch size. you can/should use broadcast add to get the desired output.</p><pre><code class="language-julia hljs"># sin/cos based position embedding which is not trainable
pe = PositionEmbedding(10) # or PositionEmbedding(10; trainable = false)

# trainable position embedding need to specify the maximum length
pe = PositionEmbedding(10, 1024; trainable = true)

x = randn(Float32, 10, 6, 3) #fake data of shape (10, length = 6, batched_size = 3)

e = pe(x) #get the position embedding
y = x .+ e # add the position embedding to each sample</code></pre><h2 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.CompositeEmbedding-Tuple{}" href="#Transformers.Basic.CompositeEmbedding-Tuple{}"><code>Transformers.Basic.CompositeEmbedding</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">CompositeEmbedding(;postprocessor=identity, es...)</code></pre><p>composite several embedding into one embedding according the aggregate methods and apply <code>postprocessor</code> on it.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/embeds/etype.jl#L14-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.Embed" href="#Transformers.Basic.Embed"><code>Transformers.Basic.Embed</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Embed(size::Int, vocab_size::Int)</code></pre><p>The Embedding Layer, <code>size</code> is the hidden size. <code>vocab_size</code> is the number of the vocabulary. Just a wrapper for embedding matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/embeds/embed.jl#L1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.MultiheadAttention-NTuple{4, Int64}" href="#Transformers.Basic.MultiheadAttention-NTuple{4, Int64}"><code>Transformers.Basic.MultiheadAttention</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">MultiheadAttention(head::Int, is::Int, hs::Int, os::Int;
                   future::Bool=true, pdrop = 0.1)</code></pre><p>Multihead dot product Attention Layer, <code>head</code> is the number of head,  <code>is</code> is the input size, <code>hs</code> is the hidden size of input projection layer of each head,  <code>os</code> is the output size. When <code>future</code> is <code>false</code>, the k-th token can&#39;t see tokens at &gt; k.  <code>pdrop</code> is the dropout rate.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/mh_atten.jl#L33-L41">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.PositionEmbedding" href="#Transformers.Basic.PositionEmbedding"><code>Transformers.Basic.PositionEmbedding</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PositionEmbedding(size::Int, max_len::Int = 1024; trainable::Bool = false)</code></pre><p>The position embedding layer. <code>size</code> is the number of neuron. <code>max_len</code> is the maximum acceptable length of input. If is not <code>trainable</code>, <code>max_len</code> will dynamically adjust to the longest input length. If <code>trainable</code>, use a random init embedding value, otherwise use a sin/cos position encoding.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/embeds/position_embed.jl#L3-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.Positionwise" href="#Transformers.Basic.Positionwise"><code>Transformers.Basic.Positionwise</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Positionwise(layers)</code></pre><p>just like <code>Flux.Chain</code>, but reshape input to 2d and reshape back when output. Work exactly the same as <code>Flux.Chain</code> when input is 2d array.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/extend3d.jl#L26-L31">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.PwFFN" href="#Transformers.Basic.PwFFN"><code>Transformers.Basic.PwFFN</code></a> — <span class="docstring-category">Type</span></header><section><div><p>just a wrapper for two dense layer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/transformer.jl#L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.Transformer-Tuple{Int64, Int64, Int64}" href="#Transformers.Basic.Transformer-Tuple{Int64, Int64, Int64}"><code>Transformers.Basic.Transformer</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">Transformer(size::Int, head::Int, ps::Int;
            future::Bool = true, act = relu, pdrop = 0.1)
Transformer(size::Int, head::Int, hs::Int, ps::Int;
            future::Bool = true, act = relu, pdrop = 0.1)</code></pre><p>Transformer layer.</p><p><code>size</code> is the input size. if <code>hs</code> is not specify, use <code>div(size, head)</code> as the hidden size of multi-head attention.  <code>ps</code> is the hidden size &amp; <code>act</code> is the activation function of the positionwise feedforward layer.  When <code>future</code> is <code>false</code>, the k-th token can&#39;t see the j-th tokens where j &gt; k. <code>pdrop</code> is the dropout rate.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/transformer.jl#L42-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.TransformerDecoder-Tuple{Int64, Int64, Int64}" href="#Transformers.Basic.TransformerDecoder-Tuple{Int64, Int64, Int64}"><code>Transformers.Basic.TransformerDecoder</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">TransformerDecoder(size::Int, head::Int, ps::Int; act = relu, pdrop = 0.1)
TransformerDecoder(size::Int, head::Int, hs::Int, ps::Int; act = relu, pdrop = 0.1)</code></pre><p>TransformerDecoder layer. Decode the value from a Encoder.</p><p><code>size</code> is the input size. if <code>hs</code> is not specify, use <code>div(size, head)</code> as the hidden size of multi-head attention.  <code>ps</code> is the hidden size &amp; <code>act</code> is the activation function of the positionwise feedforward layer.  <code>pdrop</code> is the dropout rate.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/transformer.jl#L110-L119">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.TransformerModel" href="#Transformers.Basic.TransformerModel"><code>Transformers.Basic.TransformerModel</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TransformerModel(embed::AbstractEmbed, transformers::AbstractTransformer)
TransformerModel(
                  embed::AbstractEmbed,
                  transformers::AbstractTransformer,
                  classifier
                 )</code></pre><p>a structure for putting everything together</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/model.jl#L3-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.TransformerTextEncoder" href="#Transformers.Basic.TransformerTextEncoder"><code>Transformers.Basic.TransformerTextEncoder</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct TransformerTextEncoder{T&lt;:AbstractTokenizer, V&lt;:AbstractVocabulary{String}, P} &lt;: AbstractTextEncoder
    tokenizer::T
    vocab::V
    process::P
    startsym::String
    endsym::String
    padsym::String
    trunc::Union{Nothing, Int}
end</code></pre><p>The text encoder for general transformers. Taking a tokenizer, vocabulary, and a processing function, configured with  a start symbol, an end symbol, a padding symbol, and a maximum length.</p><pre><code class="nohighlight hljs">TransformerTextEncoder(tokenze, vocab, process; trunc = nothing,
                       startsym = &quot;&lt;s&gt;&quot;, endsym = &quot;&lt;/s&gt;&quot;, unksym = &quot;&lt;unk&gt;&quot;, padsym = &quot;&lt;pad&gt;&quot;)</code></pre><p><code>tokenize</code> can be any tokenize function from <code>WordTokenizers</code>. <code>vocab</code> is either a list of word or a <code>Vocab</code>.  <code>process</code> can be omitted, then a predefined processing pipeline will be used.</p><pre><code class="nohighlight hljs">TransformerTextEncoder(f, e::TransformerTextEncoder)</code></pre><p>Take a text encoder and create a new text encoder with same configuration except the processing function.  <code>f</code> is a function that take the encoder and return a new process function. This is useful for changing part of  the procssing function.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; textenc = TransformerTextEncoder(labels; startsym, endsym, unksym,
                                        padsym = unksym, trunc = 100)
TransformerTextEncoder(
├─ TextTokenizer(default),
├─ vocab = Vocab{String, SizedArray}(size = 37678, unk = &lt;/unk&gt;, unki = 1),
├─ startsym = &lt;s&gt;,
├─ endsym = &lt;/s&gt;,
├─ padsym = &lt;/unk&gt;,
├─ trunc = 100,
└─ process = Pipelines:
  ╰─ target[tok] := nestedcall(string_getvalue, source)
  ╰─ target[tok] := with_head_tail(&lt;s&gt;, &lt;/s&gt;)(target.tok)
  ╰─ target[trunc_tok] := trunc_and_pad(100, &lt;/unk&gt;)(target.tok)
  ╰─ target[trunc_len] := nestedmaxlength(target.trunc_tok)
  ╰─ target[mask] := getmask(target.tok, target.trunc_len)
  ╰─ target[tok] := nested2batch(target.trunc_tok)
  ╰─ target := (target.tok, target.mask)
)

julia&gt; Basic.TransformerTextEncoder(textenc) do enc
           Pipelines(enc.process[1:4]) |&gt; PipeGet{(:trunc_tok, :trunc_len)}()
       end
TransformerTextEncoder(
├─ TextTokenizer(default),
├─ vocab = Vocab{String, SizedArray}(size = 37678, unk = &lt;/unk&gt;, unki = 1),
├─ startsym = &lt;s&gt;,
├─ endsym = &lt;/s&gt;,
├─ padsym = &lt;/unk&gt;,
├─ trunc = 100,
└─ process = Pipelines:
  ╰─ target[tok] := nestedcall(string_getvalue, source)
  ╰─ target[tok] := with_head_tail(&lt;s&gt;, &lt;/s&gt;)(target.tok)
  ╰─ target[trunc_tok] := trunc_and_pad(100, &lt;/unk&gt;)(target.tok)
  ╰─ target[trunc_len] := nestedmaxlength(target.trunc_tok)
  ╰─ target := (target.trunc_tok, target.trunc_len)
)
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/embeds/textencoder.jl#L24-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.Vocabulary" href="#Transformers.Basic.Vocabulary"><code>Transformers.Basic.Vocabulary</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Vocabulary{T}(voc::Vector{T}, unk::T) where T</code></pre><p>struct for holding the vocabulary list to encode/decode input tokens.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/embeds/vocab.jl#L5-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.Vocabulary-Tuple" href="#Transformers.Basic.Vocabulary-Tuple"><code>Transformers.Basic.Vocabulary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(vocab::Vocabulary)(x)</code></pre><p>encode the given data to the index encoding.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/embeds/vocab.jl#L40-L44">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="TextEncodeBase.decode-Tuple{TransformerTextEncoder, Any}" href="#TextEncodeBase.decode-Tuple{TransformerTextEncoder, Any}"><code>TextEncodeBase.decode</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">decode(textenc::TransformerTextEncoder, x)</code></pre><p>Equivalent to <code>lookup(textenc.vocab, x)</code>.</p><p>See also: <a href="#TextEncodeBase.encode-Tuple{TransformerTextEncoder, Any}"><code>encode</code></a></p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; textenc = TransformerTextEncoder(split, map(string, 1:10));

julia&gt; e = encode(textenc, [&quot;1 2 3 4 5 6 7&quot;, join(rand(1:10 , 9), &#39; &#39;)]);

julia&gt; decode(textenc, e.tok)
11×2 Matrix{String}:
 &quot;&lt;s&gt;&quot;    &quot;&lt;s&gt;&quot;
 &quot;1&quot;      &quot;3&quot;
 &quot;2&quot;      &quot;5&quot;
 &quot;3&quot;      &quot;4&quot;
 &quot;4&quot;      &quot;6&quot;
 &quot;5&quot;      &quot;6&quot;
 &quot;6&quot;      &quot;5&quot;
 &quot;7&quot;      &quot;5&quot;
 &quot;&lt;/s&gt;&quot;   &quot;6&quot;
 &quot;&lt;pad&gt;&quot;  &quot;6&quot;
 &quot;&lt;pad&gt;&quot;  &quot;&lt;/s&gt;&quot;

julia&gt; lookup(textenc.vocab, e.tok)
11×2 Matrix{String}:
 &quot;&lt;s&gt;&quot;    &quot;&lt;s&gt;&quot;
 &quot;1&quot;      &quot;3&quot;
 &quot;2&quot;      &quot;5&quot;
 &quot;3&quot;      &quot;4&quot;
 &quot;4&quot;      &quot;6&quot;
 &quot;5&quot;      &quot;6&quot;
 &quot;6&quot;      &quot;5&quot;
 &quot;7&quot;      &quot;5&quot;
 &quot;&lt;/s&gt;&quot;   &quot;6&quot;
 &quot;&lt;pad&gt;&quot;  &quot;6&quot;
 &quot;&lt;pad&gt;&quot;  &quot;&lt;/s&gt;&quot;
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/embeds/textencoder.jl#L210-L252">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="TextEncodeBase.encode-Tuple{TransformerTextEncoder, Any}" href="#TextEncodeBase.encode-Tuple{TransformerTextEncoder, Any}"><code>TextEncodeBase.encode</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">encode(::TransformerTextEncoder, ::String)</code></pre><p>Encode a single sentence with bert text encoder. The default pipeline returning  <code>@NamedTuple{tok::OneHotArray{K, 2}, mask::Nothing}</code></p><pre><code class="nohighlight hljs">encode(::TransformerTextEncoder, ::Vector{String})</code></pre><p>Encode a batch of sentences with bert text encoder. The default pipeline returning  <code>@NamedTuple{tok::OneHotArray{K, 3}, mask::Array{Float32, 3}}</code></p><p>See also: <a href="#TextEncodeBase.decode-Tuple{TransformerTextEncoder, Any}"><code>decode</code></a></p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; textenc = TransformerTextEncoder(split, map(string, 1:10))
TransformerTextEncoder(
├─ TextTokenizer(WordTokenization(split_sentences = WordTokenizers.split_sentences, tokenize = split)),
├─ vocab = Vocab{String, SizedArray}(size = 14, unk = &lt;unk&gt;, unki = 2),
├─ startsym = &lt;s&gt;,
├─ endsym = &lt;/s&gt;,
├─ padsym = &lt;pad&gt;,
└─ process = Pipelines:
  ╰─ target[tok] := nestedcall(string_getvalue, source)
  ╰─ target[tok] := with_head_tail(&lt;s&gt;, &lt;/s&gt;)(target.tok)
  ╰─ target[trunc_tok] := trunc_and_pad(nothing, &lt;pad&gt;)(target.tok)
  ╰─ target[trunc_len] := nestedmaxlength(target.trunc_tok)
  ╰─ target[mask] := getmask(target.tok, target.trunc_len)
  ╰─ target[tok] := nested2batch(target.trunc_tok)
  ╰─ target := (target.tok, target.mask)
)

julia&gt; e = encode(textenc, [&quot;1 2 3 4 5 6 7&quot;, join(rand(1:10 , 9), &#39; &#39;)])
(tok = [0 0 … 1 1; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], mask = [1.0 1.0 … 0.0 0.0;;; 1.0 1.0 … 1.0 1.0])

julia&gt; typeof(e)
NamedTuple{(:tok, :mask), Tuple{OneHotArray{0x0000000e, 2, 3, Matrix{OneHot{0x0000000e}}}, Array{Float32, 3}}}
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/embeds/textencoder.jl#L168-L207">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="TextEncodeBase.encode-Tuple{Vocabulary, Any}" href="#TextEncodeBase.encode-Tuple{Vocabulary, Any}"><code>TextEncodeBase.encode</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">encode(vocab::Vocabulary, x)</code></pre><p>encode the given data to the index encoding.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/embeds/vocab.jl#L28-L32">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.clear_classifier-Tuple{TransformerModel}" href="#Transformers.Basic.clear_classifier-Tuple{TransformerModel}"><code>Transformers.Basic.clear_classifier</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">clear_classifier(model::TransformerModel)</code></pre><p>return a new TransformerModel without classifier.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/model.jl#L30-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.getmask" href="#Transformers.Basic.getmask"><code>Transformers.Basic.getmask</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">getmask(ls::Container{&lt;:Container})</code></pre><p>get the mask for batched data.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/embeds/vocab.jl#L82-L86">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.getmask-Union{Tuple{A}, Tuple{A, A}} where A&lt;:(AbstractArray{T, 3} where T)" href="#Transformers.Basic.getmask-Union{Tuple{A}, Tuple{A, A}} where A&lt;:(AbstractArray{T, 3} where T)"><code>Transformers.Basic.getmask</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">getmask(m1::A, m2::A) where A &lt;: Abstract3DTensor</code></pre><p>get the mask for the covariance matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/embeds/vocab.jl#L107-L111">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.logcrossentropy-Union{Tuple{L}, Tuple{T}, Tuple{AbstractArray{L, 3}, AbstractArray{T, 3}, Any}} where {T, L&lt;:Union{Bool, T}}" href="#Transformers.Basic.logcrossentropy-Union{Tuple{L}, Tuple{T}, Tuple{AbstractArray{L, 3}, AbstractArray{T, 3}, Any}} where {T, L&lt;:Union{Bool, T}}"><code>Transformers.Basic.logcrossentropy</code></a> — <span class="docstring-category">Method</span></header><section><div><p>compute the cross entropy with mask where p is already the log(p)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/loss.jl#L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.logcrossentropy-Union{Tuple{L}, Tuple{T}, Tuple{AbstractArray{L, 3}, AbstractArray{T, 3}}} where {T, L&lt;:Union{Bool, T}}" href="#Transformers.Basic.logcrossentropy-Union{Tuple{L}, Tuple{T}, Tuple{AbstractArray{L, 3}, AbstractArray{T, 3}}} where {T, L&lt;:Union{Bool, T}}"><code>Transformers.Basic.logcrossentropy</code></a> — <span class="docstring-category">Method</span></header><section><div><p>compute the cross entropy where p is already the log(p)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/loss.jl#L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.logkldivergence-Union{Tuple{L}, Tuple{T}, Tuple{AbstractArray{L, 3}, AbstractArray{T, 3}, Any}} where {T, L&lt;:Union{Bool, T}}" href="#Transformers.Basic.logkldivergence-Union{Tuple{L}, Tuple{T}, Tuple{AbstractArray{L, 3}, AbstractArray{T, 3}, Any}} where {T, L&lt;:Union{Bool, T}}"><code>Transformers.Basic.logkldivergence</code></a> — <span class="docstring-category">Method</span></header><section><div><p>compute the kl divergence with mask where p is already the log(p)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/loss.jl#L1">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.logkldivergence-Union{Tuple{L}, Tuple{T}, Tuple{AbstractArray{L, 3}, AbstractArray{T, 3}}} where {T, L&lt;:Union{Bool, T}}" href="#Transformers.Basic.logkldivergence-Union{Tuple{L}, Tuple{T}, Tuple{AbstractArray{L, 3}, AbstractArray{T, 3}}} where {T, L&lt;:Union{Bool, T}}"><code>Transformers.Basic.logkldivergence</code></a> — <span class="docstring-category">Method</span></header><section><div><p>compute the kl divergence where p is already the log(p)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/loss.jl#L23">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.set_classifier-Tuple{TransformerModel, Any}" href="#Transformers.Basic.set_classifier-Tuple{TransformerModel, Any}"><code>Transformers.Basic.set_classifier</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">set_classifier(model::TransformerModel, classifier)</code></pre><p>return a new TransformerModel whose classifier is set to <code>classifier</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/model.jl#L23-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Transformers.Basic.@toNd" href="#Transformers.Basic.@toNd"><code>Transformers.Basic.@toNd</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">@toNd f(x, y, z...; a=a, b=b, c=c...) n</code></pre><p>macro for calling 2-d array function on N-d array by reshape input with reshape(x, size(x, 1), :) and reshape back with reshape(out, :, input[n][2:end]...) where n is the n-th input(default=1).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/chengchingwen/Transformers.jl/blob/c5a07fe1113e81c0525c441afbf874b0d75745bd/src/basic/extend3d.jl#L6-L12">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../tutorial/">« Tutorial</a><a class="docs-footer-nextpage" href="../stacks/">Stacks »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Sunday 27 November 2022 11:17">Sunday 27 November 2022</span>. Using Julia version 1.8.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
