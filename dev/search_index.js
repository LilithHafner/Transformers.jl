var documenterSearchIndex = {"docs":
[{"location":"datasets/#Transformers.Datasets-(not-complete)","page":"Datasets","title":"Transformers.Datasets (not complete)","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"Functions for loading some common Datasets","category":"page"},{"location":"datasets/#Provide-datasets","page":"Datasets","title":"Provide datasets","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"WMT\nWMT14 (by Google Brain)\nIWSLT\nIWSLT 2016\nen <=> de\nen <=> cs\nen <=> fr\nen <=> ar\nGLUE\nCoLA\nDiagnostic\nGLUE\nMNLI\nMRPC\nQNLI\nQQP\nRTE\nSNLI\nSST\nSTS\nWNLI","category":"page"},{"location":"datasets/#example","page":"Datasets","title":"example","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"using Transformers.Datasets\nusing Transformers.Datasets.GLUE\n\ntask = GLUE.QNLI()\ndatas = dataset(Train, task)\nget_batch(datas, 4)","category":"page"},{"location":"datasets/#API-reference","page":"Datasets","title":"API reference","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"Modules=[Transformers.Datasets]\nOrder = [:type, :function, :macro]","category":"page"},{"location":"stacks/#Transformers.Stacks","page":"Stacks","title":"Transformers.Stacks","text":"","category":"section"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"Helper struct and DSL for stacking functions/layers.","category":"page"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"Take a simple encoder-decoder model construction of machine translation task. With Transformers.jl we can easily define/stack the models. ","category":"page"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"using Transformers\nusing Transformers.Basic\n\nencoder = Stack(\n    @nntopo(e → pe:(e, pe) → x → x → $N),\n    PositionEmbedding(512),\n    (e, pe) -> e .+ pe,\n    Dropout(0.1),\n    [Transformer(512, 8, 64, 2048) for i = 1:N]...\n)\n\ndecoder = Stack(\n    @nntopo((e, m, mask):e → pe:(e, pe) → t → (t:(t, m, mask) → t:(t, m, mask)) → $N:t → c),\n    PositionEmbedding(512),\n    (e, pe) -> e .+ pe,\n    Dropout(0.1),\n    [TransformerDecoder(512, 8, 64, 2048) for i = 1:N]...,\n    Positionwise(Dense(512, length(labels)), logsoftmax)\n)\n\nfunction loss(src, trg, src_mask, trg_mask)\n    label = onehot(vocab, trg)\n\n    src = embedding(src)\n    trg = embedding(trg)\n\n    mask = getmask(src_mask, trg_mask)\n\n    enc = encoder(src)\n    dec = decoder(trg, enc, mask)\n\n    loss = logkldivergence(label, dec[:, 1:end-1, :], trg_mask[:, 1:end-1, :])\nend","category":"page"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"See example folder for the complete example.","category":"page"},{"location":"stacks/#The-Stack-NNTopo-DSL","page":"Stacks","title":"The Stack NNTopo DSL","text":"","category":"section"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"Since the TransformerDecoder require more than one input, it's not convenient to use with Chain. Therefore, we implement a very simple  DSL(Domain Specific Language) to handle the function structure. You can use the @nntopo macro to define the structure then call the function  with the given model.","category":"page"},{"location":"stacks/#NNTopo-Syntax","page":"Stacks","title":"NNTopo Syntax","text":"","category":"section"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"we call the DSL NNTopo for \"Neural Network Topology\", but actually it is just used to define where the input & output should be in a sequence of  function, or the complex version of the |> function in Julia.","category":"page"},{"location":"stacks/#\"Chain\"-the-functions","page":"Stacks","title":"\"Chain\" the functions","text":"","category":"section"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"For example:","category":"page"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"y = h(f(g(x))) #a chain of function call\n\n# or \na = g(x)\nb = f(a)\ny = h(b)\n\n# is equivalent to \ntopo = @nntopo x => a => b => y # first we define the topology/architecture\ny = topo((g, f, h), x) #then call on the given functions","category":"page"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"each => is a function call, left hand side is the input argument and right hand side is the output name.","category":"page"},{"location":"stacks/#Loop-unrolling","page":"Stacks","title":"Loop unrolling","text":"","category":"section"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"you can also unroll a loop:","category":"page"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"y = g(f(f(f(f(x)))))\n\n# or \ntmp = x\nfor i = 1:4\n  tmp = f(tmp)\nend\ny = g(tmp)\n\n# is equivalent to \ntopo = @nntopo x => 4 => y\ny = topo((f,f,f,f, g), x) # f can also be different","category":"page"},{"location":"stacks/#Multiple-argument-and-jump-connection","page":"Stacks","title":"Multiple argument & jump connection","text":"","category":"section"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"As we metioned above, the original intention was to handle the case that we have more than one input & output. So, we can do this with the following syntax: ","category":"page"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"# a complex structure\n# x1 to x4 in the given inputs\nt = f(x1, x2)\nz1, z2 = g(t, x3)\nw = h(x4, z1)\ny = k(x2, z2, w)\n\n# is equivalent to \ntopo = @nntopo (x1, x2, x3, x4):(x1, x2) => t:(t, x3) => (z1, z2):(x4, z1) => w:(x2, z2, w) => y\ny = topo((f, g, h, k), x1, x2, x3, x4)\n\n# you can also see the function with `print_topo` function\nusing Transformers.Basic: print_topo\n\nprint_topo(topo; models=(f, g, h, k))\n# \n# NNTopo{\"(x1, x2, x3, x4):(x1, x2) => (t:(t, x3) => ((z1, z2):(x4, z1) => (w:(x2, z2, w) => y)))\"}\n# topo_func(model, x1, x2, x3, x4)\n#         t = f(x1, x2)\n#         (z1, z2) = g(t, x3)\n#         w = h(x4, z1)\n#         y = k(x2, z2, w)\n#         y\n# end","category":"page"},{"location":"stacks/#Specify-the-variables-you-want","page":"Stacks","title":"Specify the variables you want","text":"","category":"section"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"Notice that we use a : to seperate the input/output variables name for each function call, if the : is not present, we will by default assume  the output variables are all the inputs of the next function call. i.e. x => (t1, t2) => y is equal to x => (t1, t2):(t1, t2) => y. ","category":"page"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"We can also return multiple variables, so the complete syntax can be viewed as:","category":"page"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"    (input arguments):(function1 inputs) => (function1 outputs):(function2 inputs):(function2 outputs) => .... => (function_n outputs):(return variables)","category":"page"},{"location":"stacks/#Interpolation","page":"Stacks","title":"Interpolation","text":"","category":"section"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"we also support interpolation, so you can use a variable to hold a substructure or the unroll number. But notice that the  interpolation variable should always be at the top level of the module since we can only get that value with eval. To use  interpolte local variables, use @nntopo_str \"topo_pattern\" instead.","category":"page"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"N = 3\ntopo = @nntopo((e, m, mask):e → pe:(e, pe) → t → (t:(t, m, mask) → t:(t, m, mask)) → $N:t → c)\n\n# or\n# topo = @nntopo_str \"(e, m, mask):e → pe:(e, pe) → t → (t:(t, m, mask) → t:(t, m, mask)) → $N:t → c\"\n\nprint_topo(topo)\n# \n# NNTopo{\"(e, m, mask):e → (pe:(e, pe) → (t → ((t:(t, m, mask) → t:(t, m, mask)) → (3:t → c))))\"}\n# topo_func(model, e, m, mask)\n#         pe = model[1](e)\n#         t = model[2](e, pe)\n#         t = model[3](t)\n#         t = model[4](t, m, mask)\n#         t = model[5](t, m, mask)\n#         t = model[6](t, m, mask)\n#         c = model[7](t)\n#         c\n# end","category":"page"},{"location":"stacks/#Nested-Structure","page":"Stacks","title":"Nested Structure","text":"","category":"section"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"you can also use the () to create a nested structure for the unroll.","category":"page"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"topo = @nntopo x => ((y => z => t) => 3 => w) => 2\nprint_topo(topo)\n# \n# NNTopo{\"x => (((y => (z => t)) => (3 => w)) => 2)\"}\n# topo_func(model, x)\n#         y = model[1](x)\n#         z = model[2](y)\n#         t = model[3](z)\n#         z = model[4](t)\n#         t = model[5](z)\n#         z = model[6](t)\n#         t = model[7](z)\n#         w = model[8](t)\n#         z = model[9](w)\n#         t = model[10](z)\n#         z = model[11](t)\n#         t = model[12](z)\n#         z = model[13](t)\n#         t = model[14](z)\n#         w = model[15](t)\n#         w\n# end","category":"page"},{"location":"stacks/#Collect-Variables","page":"Stacks","title":"Collect Variables","text":"","category":"section"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"you can also collect some variables that you are interested in with ' on that variable. For example:","category":"page"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"julia> @nntopo x => y' => 3 => z\nNNTopo{\"x => (y' => (3 => z))\"}\ntopo_func(model, x)\n        y = model[1](x)\n        %1 = y\n        y = model[2](y)\n        %2 = y\n        y = model[3](y)\n        %3 = y\n        y = model[4](y)\n        %4 = y\n        z = model[5](y)\n        (z, (%1, %2, %3, %4))\nend\n\njulia> @nntopo (x,y) => (a,b,c,d') => (w',r',y) => (m,n)' => z\nNNTopo{\"(x, y) => ((a, b, c, d') => ((w', r', y) => (((m, n))' => z)))\"}\ntopo_func(model, x, y)\n        (a, b, c, d) = model[1](x, y)\n        %1 = d\n        (w, r, y) = model[2](a, b, c, d)\n        %2 = (w, r)\n        (m, n) = model[3](w, r, y)\n        %3 = (m, n)\n        z = model[4](m, n)\n        (z, (%1, %2, %3))\nend","category":"page"},{"location":"stacks/#Stack","page":"Stacks","title":"Stack","text":"","category":"section"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"With the NNTopo DSL, now we can simple use the NNTopo with our Stack type, which is also like the Chain but we also need to pass in the  topo for the architecture. You can check the actual function call with show_stackfunc.","category":"page"},{"location":"stacks/","page":"Stacks","title":"Stacks","text":"#The Decoder Example in Attention is All you need\nusing Transformers.Stacks\nStack(\n@nntopo((e, m, mask):e → pe:(e, pe) → t → (t:(t, m, mask) → t:(t, m, mask)) → $N:t → c),\nPositionEmbedding(512),\n(e, pe) -> e .+ pe,\nDropout(0.1),\n[TransformerDecoder(512, 8, 64, 2048) for i = 1:N]...,\nPositionwise(Dense(512, length(labels)), logsoftmax)\n)\n\njulia> show_stackfunc(s)\ntopo_func(model, e, m, mask)\n        pe = PositionEmbedding(512)(e)\n        t = getfield(Main, Symbol(\"##23#25\"))()(e, pe)\n        t = Dropout{Float64}(0.1, true)(t)\n        t = TransformerDecoder(head=8, head_size=64, pwffn_size=2048, size=512, dropout=0.1)(t, m, mask)\n        t = TransformerDecoder(head=8, head_size=64, pwffn_size=2048, size=512, dropout=0.1)(t, m, mask)\n        t = TransformerDecoder(head=8, head_size=64, pwffn_size=2048, size=512, dropout=0.1)(t, m, mask)\n        c = Positionwise{Tuple{Dense{typeof(identity),TrackedArray{…,Array{Float32,2}},TrackedArray{…,Array{Float32,1}}},typeof(logsoftmax)}}((Dense(512, 12), NNlib.logsoftmax))(t)\n        c\nend","category":"page"},{"location":"pretrain/#Transformers.Pretrain","page":"Pretrain","title":"Transformers.Pretrain","text":"","category":"section"},{"location":"pretrain/","page":"Pretrain","title":"Pretrain","text":"Functions for download and loading pretrain models.","category":"page"},{"location":"pretrain/#using-Pretrains","page":"Pretrain","title":"using Pretrains","text":"","category":"section"},{"location":"pretrain/","page":"Pretrain","title":"Pretrain","text":"For GPT and BERT, we provide a simple api to get the released pretrain weight and load them into our Julia version Transformer implementation. ","category":"page"},{"location":"pretrain/","page":"Pretrain","title":"Pretrain","text":"using Transformers\nusing Transformers.Pretrain\nusing Transformers.GenerativePreTrain\nusing Transformers.BidirectionalEncoder\n\n# disable cli download check\nENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true\n\n#load everything in the pretrain model\nbert_model, wordpiece, tokenizer = pretrain\"Bert-uncased_L-12_H-768_A-12\" \n\n#load model weight only\ngpt_model = pretrain\"gpt-OpenAIftlm:gpt_model\"\n\n#show the loaded model\nshow(bert_model)\nshow(gpt_model)","category":"page"},{"location":"pretrain/","page":"Pretrain","title":"Pretrain","text":"The pretrain\"<type>-<model-name>:<item>\" string with pretrain prefix will load the specific item from a known pretrain file (see the list below).  The <type> is matched case insensitively, so not matter bert, Bert, BERT, or even bErT will find the BERT pretrain model. On the other hand,  the <model-name>, and <item> should be exactly the one on the list. See example.","category":"page"},{"location":"pretrain/","page":"Pretrain","title":"Pretrain","text":"Currently support pretrain:","category":"page"},{"location":"pretrain/","page":"Pretrain","title":"Pretrain","text":"using Transformers.Pretrain\nPretrain.pretrains(; detailed=true)","category":"page"},{"location":"pretrain/","page":"Pretrain","title":"Pretrain","text":"If you don't find a public pretrain you want on the list, please fire an issue.","category":"page"},{"location":"pretrain/","page":"Pretrain","title":"Pretrain","text":"See example folder for the complete example.","category":"page"},{"location":"pretrain/#API-reference","page":"Pretrain","title":"API reference","text":"","category":"section"},{"location":"pretrain/","page":"Pretrain","title":"Pretrain","text":"Modules=[Transformers.Pretrain]\nOrder = [:type, :function, :macro]","category":"page"},{"location":"pretrain/#Transformers.Pretrain.load_pretrain-Tuple{Any}","page":"Pretrain","title":"Transformers.Pretrain.load_pretrain","text":"load_pretrain(name; kw...)\n\nsame as @pretrain_str, but can pass keyword argument if needed.\n\n\n\n\n\n","category":"method"},{"location":"pretrain/#Transformers.Pretrain.pretrains","page":"Pretrain","title":"Transformers.Pretrain.pretrains","text":"pretrains(query::String = \"\"; detailed::Bool = false)\n\nShow all available models. you can also query a specific model or model name. show more detail with detailed = true.\n\n\n\n\n\n","category":"function"},{"location":"pretrain/#Transformers.Pretrain.@pretrain_str-Tuple{Any}","page":"Pretrain","title":"Transformers.Pretrain.@pretrain_str","text":"pretrain\"model-description:item\"\n\nconvenient macro for loading data from pretrain. Use DataDeps to automatically download if a model is not found. the string should be in pretrain\"<type>-<model-name>:<item>\" format.\n\nsee also Pretrain.pretrains().\n\n\n\n\n\n","category":"macro"},{"location":"basic/#Transformers.Basic","page":"Basic","title":"Transformers.Basic","text":"","category":"section"},{"location":"basic/","page":"Basic","title":"Basic","text":"Basic functionality of Transformers.jl, provide the Transformer encoder/decoder implementation and other convenient function.","category":"page"},{"location":"basic/#Transformer","page":"Basic","title":"Transformer","text":"","category":"section"},{"location":"basic/","page":"Basic","title":"Basic","text":"The Transformer and TransformerDecoder is the encoder and decoder block of the origin paper, and they are all implement as the  regular Flux Layer, so you can treat them just as the Dense layer. See the docstring for the argument. However, for the sequence  data input, we usually have a 3 dimensional input of shape (hidden size, sequence length, batch size) instead of just (hidden size, batch size).  Therefore, we implement both 2d & 3d operation according to the input type (The N of Array{T, N}). We are able to handle both input of shape  (hidden size, sequence length, batch size) and (hidden size, sequence length) for the case with only 1 input.","category":"page"},{"location":"basic/","page":"Basic","title":"Basic","text":"using Transformers\n\nm = Transformer(512, 8, 64, 2048) #define a Transformer block with 8 head and 64 neuron for each head\nx = randn(512, 30, 3) #fake data of length 30\n\ny = m(x)","category":"page"},{"location":"basic/#Positionwise","page":"Basic","title":"Positionwise","text":"","category":"section"},{"location":"basic/","page":"Basic","title":"Basic","text":"For the sequential task, we need to handle the 3 dimensional input. However, most of the layer in Flux only support input with shape  (hidden size, batch size). In order to tackle this problem, we implement the Positionwise helper function that is almost the same  as Flux.Chain but it will run the model position-wisely. (internally it just reshape the input to 2d and apply the model then reshape  back). ","category":"page"},{"location":"basic/","page":"Basic","title":"Basic","text":"using Transformers\nusing Flux\n\nm = Positionwise(Dense(10, 5), Dense(5, 2), softmax)\nx = randn(10, 30, 3)\n\ny = m(x)\n\n# which is equivalent to \n# \n# m = Chain(Dense(10, 5), Dense(5, 2), softmax)\n# x1 = randn(10, 30)\n# x2 = randn(10, 30)\n# x3 = randn(10, 30)\n# y = cat(m(x1), m(x2), m(x3); dims=3)","category":"page"},{"location":"basic/#PositionEmbedding","page":"Basic","title":"PositionEmbedding","text":"","category":"section"},{"location":"basic/","page":"Basic","title":"Basic","text":"We implement two kinds of position embedding, one is based on the sin/cos function (mentioned in the paper,  attention is all you need). Another one is just like regular word embedding but with the position index. The  first argument is the size. Since the position embedding is only related to the length of the input ( we use size(input, 2) as the length), the return value of the layer will be the embedding of the given  length without duplicate to the batch size. you can/should use broadcast add to get the desired output.","category":"page"},{"location":"basic/","page":"Basic","title":"Basic","text":"# sin/cos based position embedding which is not trainable\npe = PositionEmbedding(10) # or PositionEmbedding(10; trainable = false)\n\n# trainable position embedding need to specify the maximum length\npe = PositionEmbedding(10, 1024; trainable = true)\n\nx = randn(Float32, 10, 6, 3) #fake data of shape (10, length = 6, batched_size = 3)\n\ne = pe(x) #get the position embedding\ny = x .+ e # add the position embedding to each sample","category":"page"},{"location":"basic/#API-Reference","page":"Basic","title":"API Reference","text":"","category":"section"},{"location":"basic/","page":"Basic","title":"Basic","text":"Modules=[Transformers.Basic]\nOrder = [:type, :function, :macro]","category":"page"},{"location":"basic/#Transformers.Basic.CompositeEmbedding-Tuple{}","page":"Basic","title":"Transformers.Basic.CompositeEmbedding","text":"CompositeEmbedding(;postprocessor=identity, es...)\n\ncomposite several embedding into one embedding according the aggregate methods and apply postprocessor on it.\n\n\n\n\n\n","category":"method"},{"location":"basic/#Transformers.Basic.Embed","page":"Basic","title":"Transformers.Basic.Embed","text":"Embed(size::Int, vocab_size::Int)\n\nThe Embedding Layer, size is the hidden size. vocab_size is the number of the vocabulary. Just a wrapper for embedding matrix.\n\n\n\n\n\n","category":"type"},{"location":"basic/#Transformers.Basic.MultiheadAttention-NTuple{4, Int64}","page":"Basic","title":"Transformers.Basic.MultiheadAttention","text":"MultiheadAttention(head::Int, is::Int, hs::Int, os::Int;\n                   future::Bool=true, pdrop = 0.1)\n\nMultihead dot product Attention Layer, head is the number of head,  is is the input size, hs is the hidden size of input projection layer of each head,  os is the output size. When future is false, the k-th token can't see tokens at > k.  pdrop is the dropout rate.\n\n\n\n\n\n","category":"method"},{"location":"basic/#Transformers.Basic.PositionEmbedding","page":"Basic","title":"Transformers.Basic.PositionEmbedding","text":"PositionEmbedding(size::Int, max_len::Int = 1024; trainable::Bool = false)\n\nThe position embedding layer. size is the number of neuron. max_len is the maximum acceptable length of input. If is not trainable, max_len will dynamically adjust to the longest input length. If trainable, use a random init embedding value, otherwise use a sin/cos position encoding.\n\n\n\n\n\n","category":"type"},{"location":"basic/#Transformers.Basic.Positionwise","page":"Basic","title":"Transformers.Basic.Positionwise","text":"Positionwise(layers)\n\njust like Flux.Chain, but reshape input to 2d and reshape back when output. Work exactly the same as Flux.Chain when input is 2d array.\n\n\n\n\n\n","category":"type"},{"location":"basic/#Transformers.Basic.PwFFN","page":"Basic","title":"Transformers.Basic.PwFFN","text":"just a wrapper for two dense layer.\n\n\n\n\n\n","category":"type"},{"location":"basic/#Transformers.Basic.Transformer-Tuple{Int64, Int64, Int64}","page":"Basic","title":"Transformers.Basic.Transformer","text":"Transformer(size::Int, head::Int, ps::Int;\n            future::Bool = true, act = relu, pdrop = 0.1)\nTransformer(size::Int, head::Int, hs::Int, ps::Int;\n            future::Bool = true, act = relu, pdrop = 0.1)\n\nTransformer layer.\n\nsize is the input size. if hs is not specify, use div(size, head) as the hidden size of multi-head attention.  ps is the hidden size & act is the activation function of the positionwise feedforward layer.  When future is false, the k-th token can't see the j-th tokens where j > k. pdrop is the dropout rate.\n\n\n\n\n\n","category":"method"},{"location":"basic/#Transformers.Basic.TransformerDecoder-Tuple{Int64, Int64, Int64}","page":"Basic","title":"Transformers.Basic.TransformerDecoder","text":"TransformerDecoder(size::Int, head::Int, ps::Int; act = relu, pdrop = 0.1)\nTransformerDecoder(size::Int, head::Int, hs::Int, ps::Int; act = relu, pdrop = 0.1)\n\nTransformerDecoder layer. Decode the value from a Encoder.\n\nsize is the input size. if hs is not specify, use div(size, head) as the hidden size of multi-head attention.  ps is the hidden size & act is the activation function of the positionwise feedforward layer.  pdrop is the dropout rate.\n\n\n\n\n\n","category":"method"},{"location":"basic/#Transformers.Basic.TransformerModel","page":"Basic","title":"Transformers.Basic.TransformerModel","text":"TransformerModel(embed::AbstractEmbed, transformers::AbstractTransformer)\nTransformerModel(\n                  embed::AbstractEmbed,\n                  transformers::AbstractTransformer,\n                  classifier\n                 )\n\na structure for putting everything together\n\n\n\n\n\n","category":"type"},{"location":"basic/#Transformers.Basic.TransformerTextEncoder","page":"Basic","title":"Transformers.Basic.TransformerTextEncoder","text":"struct TransformerTextEncoder{T<:AbstractTokenizer, V<:AbstractVocabulary{String}, P} <: AbstractTextEncoder\n    tokenizer::T\n    vocab::V\n    process::P\n    startsym::String\n    endsym::String\n    padsym::String\n    trunc::Union{Nothing, Int}\nend\n\nThe text encoder for general transformers. Taking a tokenizer, vocabulary, and a processing function, configured with  a start symbol, an end symbol, a padding symbol, and a maximum length.\n\nTransformerTextEncoder(tokenze, vocab, process; trunc = nothing,\n                       startsym = \"<s>\", endsym = \"</s>\", unksym = \"<unk>\", padsym = \"<pad>\")\n\ntokenize can be any tokenize function from WordTokenizers. vocab is either a list of word or a Vocab.  process can be omitted, then a predefined processing pipeline will be used.\n\nTransformerTextEncoder(f, e::TransformerTextEncoder)\n\nTake a text encoder and create a new text encoder with same configuration except the processing function.  f is a function that take the encoder and return a new process function. This is useful for changing part of  the procssing function.\n\nExample\n\njulia> textenc = TransformerTextEncoder(labels; startsym, endsym, unksym,\n                                        padsym = unksym, trunc = 100)\nTransformerTextEncoder(\n├─ TextTokenizer(default),\n├─ vocab = Vocab{String, SizedArray}(size = 37678, unk = </unk>, unki = 1),\n├─ startsym = <s>,\n├─ endsym = </s>,\n├─ padsym = </unk>,\n├─ trunc = 100,\n└─ process = Pipelines:\n  ╰─ target[tok] := nestedcall(string_getvalue, source)\n  ╰─ target[tok] := with_head_tail(<s>, </s>)(target.tok)\n  ╰─ target[trunc_tok] := trunc_and_pad(100, </unk>)(target.tok)\n  ╰─ target[trunc_len] := nestedmaxlength(target.trunc_tok)\n  ╰─ target[mask] := getmask(target.tok, target.trunc_len)\n  ╰─ target[tok] := nested2batch(target.trunc_tok)\n  ╰─ target := (target.tok, target.mask)\n)\n\njulia> Basic.TransformerTextEncoder(textenc) do enc\n           Pipelines(enc.process[1:4]) |> PipeGet{(:trunc_tok, :trunc_len)}()\n       end\nTransformerTextEncoder(\n├─ TextTokenizer(default),\n├─ vocab = Vocab{String, SizedArray}(size = 37678, unk = </unk>, unki = 1),\n├─ startsym = <s>,\n├─ endsym = </s>,\n├─ padsym = </unk>,\n├─ trunc = 100,\n└─ process = Pipelines:\n  ╰─ target[tok] := nestedcall(string_getvalue, source)\n  ╰─ target[tok] := with_head_tail(<s>, </s>)(target.tok)\n  ╰─ target[trunc_tok] := trunc_and_pad(100, </unk>)(target.tok)\n  ╰─ target[trunc_len] := nestedmaxlength(target.trunc_tok)\n  ╰─ target := (target.trunc_tok, target.trunc_len)\n)\n\n\n\n\n\n\n","category":"type"},{"location":"basic/#Transformers.Basic.Vocabulary","page":"Basic","title":"Transformers.Basic.Vocabulary","text":"Vocabulary{T}(voc::Vector{T}, unk::T) where T\n\nstruct for holding the vocabulary list to encode/decode input tokens.\n\n\n\n\n\n","category":"type"},{"location":"basic/#Transformers.Basic.Vocabulary-Tuple","page":"Basic","title":"Transformers.Basic.Vocabulary","text":"(vocab::Vocabulary)(x)\n\nencode the given data to the index encoding.\n\n\n\n\n\n","category":"method"},{"location":"basic/#TextEncodeBase.decode-Tuple{TransformerTextEncoder, Any}","page":"Basic","title":"TextEncodeBase.decode","text":"decode(textenc::TransformerTextEncoder, x)\n\nEquivalent to lookup(textenc.vocab, x).\n\nSee also: encode\n\nExample\n\njulia> textenc = TransformerTextEncoder(split, map(string, 1:10));\n\njulia> e = encode(textenc, [\"1 2 3 4 5 6 7\", join(rand(1:10 , 9), ' ')]);\n\njulia> decode(textenc, e.tok)\n11×2 Matrix{String}:\n \"<s>\"    \"<s>\"\n \"1\"      \"3\"\n \"2\"      \"5\"\n \"3\"      \"4\"\n \"4\"      \"6\"\n \"5\"      \"6\"\n \"6\"      \"5\"\n \"7\"      \"5\"\n \"</s>\"   \"6\"\n \"<pad>\"  \"6\"\n \"<pad>\"  \"</s>\"\n\njulia> lookup(textenc.vocab, e.tok)\n11×2 Matrix{String}:\n \"<s>\"    \"<s>\"\n \"1\"      \"3\"\n \"2\"      \"5\"\n \"3\"      \"4\"\n \"4\"      \"6\"\n \"5\"      \"6\"\n \"6\"      \"5\"\n \"7\"      \"5\"\n \"</s>\"   \"6\"\n \"<pad>\"  \"6\"\n \"<pad>\"  \"</s>\"\n\n\n\n\n\n\n","category":"method"},{"location":"basic/#TextEncodeBase.encode-Tuple{TransformerTextEncoder, Any}","page":"Basic","title":"TextEncodeBase.encode","text":"encode(::TransformerTextEncoder, ::String)\n\nEncode a single sentence with bert text encoder. The default pipeline returning  @NamedTuple{tok::OneHotArray{K, 2}, mask::Nothing}\n\nencode(::TransformerTextEncoder, ::Vector{String})\n\nEncode a batch of sentences with bert text encoder. The default pipeline returning  @NamedTuple{tok::OneHotArray{K, 3}, mask::Array{Float32, 3}}\n\nSee also: decode\n\nExample\n\njulia> textenc = TransformerTextEncoder(split, map(string, 1:10))\nTransformerTextEncoder(\n├─ TextTokenizer(WordTokenization(split_sentences = WordTokenizers.split_sentences, tokenize = split)),\n├─ vocab = Vocab{String, SizedArray}(size = 14, unk = <unk>, unki = 2),\n├─ startsym = <s>,\n├─ endsym = </s>,\n├─ padsym = <pad>,\n└─ process = Pipelines:\n  ╰─ target[tok] := nestedcall(string_getvalue, source)\n  ╰─ target[tok] := with_head_tail(<s>, </s>)(target.tok)\n  ╰─ target[trunc_tok] := trunc_and_pad(nothing, <pad>)(target.tok)\n  ╰─ target[trunc_len] := nestedmaxlength(target.trunc_tok)\n  ╰─ target[mask] := getmask(target.tok, target.trunc_len)\n  ╰─ target[tok] := nested2batch(target.trunc_tok)\n  ╰─ target := (target.tok, target.mask)\n)\n\njulia> e = encode(textenc, [\"1 2 3 4 5 6 7\", join(rand(1:10 , 9), ' ')])\n(tok = [0 0 … 1 1; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0;;; 0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], mask = [1.0 1.0 … 0.0 0.0;;; 1.0 1.0 … 1.0 1.0])\n\njulia> typeof(e)\nNamedTuple{(:tok, :mask), Tuple{OneHotArray{0x0000000e, 2, 3, Matrix{OneHot{0x0000000e}}}, Array{Float32, 3}}}\n\n\n\n\n\n\n","category":"method"},{"location":"basic/#TextEncodeBase.encode-Tuple{Vocabulary, Any}","page":"Basic","title":"TextEncodeBase.encode","text":"encode(vocab::Vocabulary, x)\n\nencode the given data to the index encoding.\n\n\n\n\n\n","category":"method"},{"location":"basic/#Transformers.Basic.clear_classifier-Tuple{TransformerModel}","page":"Basic","title":"Transformers.Basic.clear_classifier","text":"clear_classifier(model::TransformerModel)\n\nreturn a new TransformerModel without classifier.\n\n\n\n\n\n","category":"method"},{"location":"basic/#Transformers.Basic.getmask","page":"Basic","title":"Transformers.Basic.getmask","text":"getmask(ls::Container{<:Container})\n\nget the mask for batched data.\n\n\n\n\n\n","category":"function"},{"location":"basic/#Transformers.Basic.getmask-Union{Tuple{A}, Tuple{A, A}} where A<:(AbstractArray{T, 3} where T)","page":"Basic","title":"Transformers.Basic.getmask","text":"getmask(m1::A, m2::A) where A <: Abstract3DTensor\n\nget the mask for the covariance matrix.\n\n\n\n\n\n","category":"method"},{"location":"basic/#Transformers.Basic.logcrossentropy-Union{Tuple{L}, Tuple{T}, Tuple{AbstractArray{L, 3}, AbstractArray{T, 3}, Any}} where {T, L<:Union{Bool, T}}","page":"Basic","title":"Transformers.Basic.logcrossentropy","text":"compute the cross entropy with mask where p is already the log(p)\n\n\n\n\n\n","category":"method"},{"location":"basic/#Transformers.Basic.logcrossentropy-Union{Tuple{L}, Tuple{T}, Tuple{AbstractArray{L, 3}, AbstractArray{T, 3}}} where {T, L<:Union{Bool, T}}","page":"Basic","title":"Transformers.Basic.logcrossentropy","text":"compute the cross entropy where p is already the log(p)\n\n\n\n\n\n","category":"method"},{"location":"basic/#Transformers.Basic.logkldivergence-Union{Tuple{L}, Tuple{T}, Tuple{AbstractArray{L, 3}, AbstractArray{T, 3}, Any}} where {T, L<:Union{Bool, T}}","page":"Basic","title":"Transformers.Basic.logkldivergence","text":"compute the kl divergence with mask where p is already the log(p)\n\n\n\n\n\n","category":"method"},{"location":"basic/#Transformers.Basic.logkldivergence-Union{Tuple{L}, Tuple{T}, Tuple{AbstractArray{L, 3}, AbstractArray{T, 3}}} where {T, L<:Union{Bool, T}}","page":"Basic","title":"Transformers.Basic.logkldivergence","text":"compute the kl divergence where p is already the log(p)\n\n\n\n\n\n","category":"method"},{"location":"basic/#Transformers.Basic.set_classifier-Tuple{TransformerModel, Any}","page":"Basic","title":"Transformers.Basic.set_classifier","text":"set_classifier(model::TransformerModel, classifier)\n\nreturn a new TransformerModel whose classifier is set to classifier.\n\n\n\n\n\n","category":"method"},{"location":"basic/#Transformers.Basic.@toNd","page":"Basic","title":"Transformers.Basic.@toNd","text":"@toNd f(x, y, z...; a=a, b=b, c=c...) n\n\nmacro for calling 2-d array function on N-d array by reshape input with reshape(x, size(x, 1), :) and reshape back with reshape(out, :, input[n][2:end]...) where n is the n-th input(default=1).\n\n\n\n\n\n","category":"macro"},{"location":"gpt/#Transformers.GenerativePreTrain","page":"GPT","title":"Transformers.GenerativePreTrain","text":"","category":"section"},{"location":"gpt/","page":"GPT","title":"GPT","text":"Implementation of gpt-1 model","category":"page"},{"location":"gpt/#API-reference","page":"GPT","title":"API reference","text":"","category":"section"},{"location":"gpt/","page":"GPT","title":"GPT","text":"Modules=[Transformers.GenerativePreTrain]\nOrder = [:type, :function, :macro]","category":"page"},{"location":"gpt/#Transformers.GenerativePreTrain.Gpt-NTuple{4, Int64}","page":"GPT","title":"Transformers.GenerativePreTrain.Gpt","text":"Gpt(size::Int, head::Int, ps::Int, layer::Int;\n    act = gelu, pdrop = 0.1, attn_pdrop = 0.1)\nGpt(size::Int, head::Int, hs::Int, ps::Int, layer::Int;\n    act = gelu, pdrop = 0.1, attn_pdrop = 0.1)\n\nthe Generative Pretrained Transformer(GPT) model.\n\n(gpt::Gpt)(x::T, mask=nothing; all::Bool=false)\n\neval the gpt layer on input x. If length mask is given (in shape (1, seqlen, batchsize)), mask the attention with mask. Moreover, set all to true to get all  outputs of each transformer layer.\n\n\n\n\n\n","category":"method"},{"location":"gpt/#Transformers.GenerativePreTrain.gpt_tokenizer-Tuple{Any}","page":"GPT","title":"Transformers.GenerativePreTrain.gpt_tokenizer","text":"gpt_tokenizer(x)\n\nAn alternative for origin tokenizer (spacy tokenizer) used in gpt model.\n\n\n\n\n\n","category":"method"},{"location":"gpt/#Transformers.GenerativePreTrain.lmloss-Union{Tuple{N}, Tuple{T}, Tuple{Embed{T, W} where W<:(AbstractArray{T}), Any, AbstractArray{T, N}, Any}} where {T, N}","page":"GPT","title":"Transformers.GenerativePreTrain.lmloss","text":"lmloss(embed, onehot, encoding, mask)\n\ncompute the language modeling loss for Gpt, onehot is the onehot array of the origin input sentence. encoding the output of Gpt, mask is the mask between input sentences.\n\n\n\n\n\n","category":"method"},{"location":"gpt/#Transformers.GenerativePreTrain.load_gpt_pretrain","page":"GPT","title":"Transformers.GenerativePreTrain.load_gpt_pretrain","text":"load_gpt_pretrain(path::AbstractString, sym = :all;\n                  startsym=\"_start_\",\n                  delisym=\"_delimiter_\",\n                  clfsym=\"_classify_\",\n                  unksym=\"<unk>\")\n\nload gpt data/model from pretrain bson data. use sym to determine which data to load.  set <xxx>syms for setting special symbols in vocabulary.\n\npossible value: :all(default), gpt_model, bpe, vocab, tokenizer.\n\n\n\n\n\n","category":"function"},{"location":"gpt/#Transformers.GenerativePreTrain.openAInpy2bson-Tuple{Any}","page":"GPT","title":"Transformers.GenerativePreTrain.openAInpy2bson","text":"openAInpy2bson(path;\n               raw=false,\n               saveto=\"./\",\n               startsym=\"_start_\",\n               delisym=\"_delimiter_\",\n               clfsym=\"_classify_\",\n               unksym=\"<unk>\")\n\nturn openai released gpt format(npy) into BSON file. Set raw to true to remain the origin data format in bson.\n\n\n\n\n\n","category":"method"},{"location":"gpt/#Transformers.GenerativePreTrain.text_standardize-Tuple{Any}","page":"GPT","title":"Transformers.GenerativePreTrain.text_standardize","text":"The function in the origin gpt code\n\nfixes some issues the spacy tokenizer had on books corpus also does some whitespace standardization\n\n\n\n\n\n","category":"method"},{"location":"#Transformers.jl","page":"Home","title":"Transformers.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Julia implementation of Transformers models","category":"page"},{"location":"","page":"Home","title":"Home","text":"This is the documentation of Transformers: The Julia solution for using Transformer models based on Flux.jl","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In the Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]add Transformers","category":"page"},{"location":"","page":"Home","title":"Home","text":"For using GPU, make sure CUDA.jl is runable on your computer:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]add CUDA; build","category":"page"},{"location":"#Implemented-model","page":"Home","title":"Implemented model","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can find the code in example folder.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Attention is all you need\nImproving Language Understanding by Generative Pre-Training\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","category":"page"},{"location":"#Example","page":"Home","title":"Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Using pretrained Bert with Transformers.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Transformers\nusing Transformers.Basic\nusing Transformers.Pretrain\n\nENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true\n\nbert_model, wordpiece, tokenizer = pretrain\"bert-uncased_L-12_H-768_A-12\"\nvocab = Vocabulary(wordpiece)\n\ntext1 = \"Peter Piper picked a peck of pickled peppers\" |> tokenizer |> wordpiece\ntext2 = \"Fuzzy Wuzzy was a bear\" |> tokenizer |> wordpiece\n\ntext = [\"[CLS]\"; text1; \"[SEP]\"; text2; \"[SEP]\"]\n@assert text == [\n    \"[CLS]\", \"peter\", \"piper\", \"picked\", \"a\", \"peck\", \"of\", \"pick\", \"##led\", \"peppers\", \"[SEP]\", \n    \"fuzzy\", \"wu\", \"##zzy\",  \"was\", \"a\", \"bear\", \"[SEP]\"\n]\n\ntoken_indices = vocab(text)\nsegment_indices = [fill(1, length(text1)+2); fill(2, length(text2)+1)]\n\nsample = (tok = token_indices, segment = segment_indices)\n\nbert_embedding = sample |> bert_model.embed\nfeature_tensors = bert_embedding |> bert_model.transformers","category":"page"},{"location":"#Module-Hierarchy","page":"Home","title":"Module Hierarchy","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Transformers.Basic","category":"page"},{"location":"","page":"Home","title":"Home","text":"Basic functionality of Transformers.jl, provide the Transformer encoder/decoder implementation and other convenient function.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Transformers.Pretrain","category":"page"},{"location":"","page":"Home","title":"Home","text":"Functions for download and loading pretrain models.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Transformers.Stacks","category":"page"},{"location":"","page":"Home","title":"Home","text":"Helper struct and DSL for stacking functions/layers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Transformers.Datasets","category":"page"},{"location":"","page":"Home","title":"Home","text":"Functions for loading some common Datasets","category":"page"},{"location":"","page":"Home","title":"Home","text":"Transformers.GenerativePreTrain","category":"page"},{"location":"","page":"Home","title":"Home","text":"Implementation of gpt-1 model","category":"page"},{"location":"","page":"Home","title":"Home","text":"Transformers.BidirectionalEncoder","category":"page"},{"location":"","page":"Home","title":"Home","text":"Implementation of BERT model","category":"page"},{"location":"#Outline","page":"Home","title":"Outline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n  \"tutorial.md\",\n  \"basic.md\",\n  \"stacks.md\",\n  \"pretrain.md\",\n  \"gpt.md\",\n  \"bert.md\",\n  \"datasets.md\",\n]\nDepth = 3","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The following content will cover the basic introductions about the Transformer model and the implementation.","category":"page"},{"location":"tutorial/#Transformer-model","page":"Tutorial","title":"Transformer model","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The Transformer model was proposed in the paper: Attention Is All You Need. In that paper they provide a new way of handling the sequence transduction problem (like the machine translation task) without complex recurrent or convolutional structure. Simply use a stack of attention mechanisms to get the latent structure in the input sentences and a special embedding (positional embedding) to get the locationality. The whole model architecture looks like this:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The Transformer model architecture (picture from the origin paper)(Image: transformer)","category":"page"},{"location":"tutorial/#Multi-Head-Attention","page":"Tutorial","title":"Multi-Head Attention","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Instead of using the regular attention mechanism, they split the input vector to several pairs of subvector and perform a dot-product attention on each subvector pairs.  ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"regular attention v.s. Multi-Head attention (picture from the origin paper)(Image: mhatten)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For those who like mathematical expression, here is the formula:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Attention(Q K V) = softmax(fracQK^Tsqrtd_k)V","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"MultiHead(Q K V) = Concat(head_1 head_h)W^O\ntextwhere head_i = Attention(QW^Q_i KW^K_i VW^V_i)","category":"page"},{"location":"tutorial/#Positional-Embedding","page":"Tutorial","title":"Positional Embedding","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"As we mentioned above, transformer model didn't depend on the recurrent or convolutional structure. On the other hand, we still need a way to differentiate two sequence with same words but different order. Therefore, they add the locational information on the embedding, i.e. the origin word embedding plus a special embedding that indicate the order of that word. The special embedding can be computed by some equations or just use another trainable embedding matrix. In the paper, the positional embedding use this formula:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"PE_(pos k) = begincases\nsin(fracpos10^4kd_k) textif k text is even\ncos(fracpos10^4kd_k)  textif k text is odd\nendcases","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"where pos is the locational information that tells you the given word is the pos-th word, and k is the k-th dimension of the input vector. d_k is the total length of the word/positional embedding. So the new embedding will be computed as:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Embedding_k(word) = WordEmbedding_k(word) + PE(pos_of_word k)","category":"page"},{"location":"tutorial/#Transformers.jl","page":"Tutorial","title":"Transformers.jl","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Now we know how the transformer model looks like, let's take a look at the Transformers.jl. The package is build on top of a famous deep learning framework in Julia, Flux.jl.","category":"page"},{"location":"tutorial/#Example","page":"Tutorial","title":"Example","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To best illustrate the usage of Transformers.jl, we will start with building a two layer Transformer model on a sequence copy task. Before we start, we need to install all the package we need:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Pkg\nPkg.add(\"CUDA\")\nPkg.add(\"Flux\")\nPkg.add(\"Transformers\")","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We use CUDA.jl for the GPU support.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Flux\nusing CUDA\nusing Transformers\nusing Transformers.Basic #for loading the positional embedding\n\nenable_gpu(true) # make `todevice` work on gpu","category":"page"},{"location":"tutorial/#Copy-task","page":"Tutorial","title":"Copy task","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The copy task is a toy test case of a sequence transduction problem that simply return the same sequence as the output. Here we define the input as a random sequence with number from 1~10 and length 10. we will also need a start and end symbol to indicate where is the begin and end of the sequence. We can use Transformers.Basic.Vocabulary to turn the input to corresponding index.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"labels = map(string, 1:10)\nstartsym = \"11\"\nendsym = \"12\"\nunksym = \"0\"\nlabels = [unksym, startsym, endsym, labels...]\nvocab = Vocabulary(labels, unksym)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"#function for generate training datas\nsample_data() = (d = map(string, rand(1:10, 10)); (d,d))\n#function for adding start & end symbol\npreprocess(x) = [startsym, x..., endsym]\n\n@show sample = preprocess.(sample_data())\n@show encoded_sample = vocab(sample[1]) #use Vocabulary to encode the training data","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"sample = preprocess.(sample_data()) = ([\"11\", \"10\", \"8\", \"1\", \"10\", \"7\", \"10\", \"4\", \"2\", \"3\", \"3\", \"12\"], [\"11\", \"10\", \"8\", \"1\", \"10\", \"7\", \"10\", \"4\", \"2\", \"3\", \"3\", \"12\"])\nencoded_sample = vocab(sample[1]) = [2, 13, 11, 4, 13, 10, 13, 7, 5, 6, 6, 3]","category":"page"},{"location":"tutorial/#Defining-the-model","page":"Tutorial","title":"Defining the model","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"With the Transformers.jl and Flux.jl, we can define the model easily. We use a Transformer with 512 hidden size and 8 head.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"#define a Word embedding layer which turn word index to word vector\nembed = Embed(512, length(vocab)) |> gpu\n#define a position embedding layer metioned above\npe = PositionEmbedding(512) |> gpu\n\n#wrapper for get embedding\nfunction embedding(x)\n  we = embed(x, inv(sqrt(512)))\n  e = we .+ pe(we)\n  return e\nend\n\n#define 2 layer of transformer\nencode_t1 = Transformer(512, 8, 64, 2048) |> gpu\nencode_t2 = Transformer(512, 8, 64, 2048) |> gpu\n\n#define 2 layer of transformer decoder\ndecode_t1 = TransformerDecoder(512, 8, 64, 2048) |> gpu\ndecode_t2 = TransformerDecoder(512, 8, 64, 2048) |> gpu\n\n#define the layer to get the final output probabilities\nlinear = Positionwise(Dense(512, length(vocab)), logsoftmax) |> gpu\n\nfunction encoder_forward(x)\n  e = embedding(x)\n  t1 = encode_t1(e)\n  t2 = encode_t2(t1)\n  return t2\nend\n\nfunction decoder_forward(x, m)\n  e = embedding(x)\n  t1 = decode_t1(e, m)\n  t2 = decode_t2(t1, m)\n  p = linear(t2)\n  return p\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Then run the model on the sample","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"enc = encoder_forward(encoded_sample)\nprobs = decoder_forward(encoded_sample, enc)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can also use theTransformers.Stack to define the encoder and decoder so you can define multiple layer and the xx_forwawrd at once. See the docs for more information about the API.","category":"page"},{"location":"tutorial/#define-the-loss-and-training-loop","page":"Tutorial","title":"define the loss and training loop","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"For the last step, we need to define the loss function and training loop. We use the kl divergence for the output probability.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Flux: onehot\nfunction smooth(et)\n    sm = fill!(similar(et, Float32), 1e-6/size(embed, 2))\n    p = sm .* (1 .+ -et)\n    label = p .+ et .* (1 - convert(Float32, 1e-6))\n    label\nend\nFlux.@nograd smooth\n\n#define loss function\nfunction loss(x, y)\n  label = onehot(vocab, y) #turn the index to one-hot encoding\n  label = smooth(label) #perform label smoothing\n  enc = encoder_forward(x)\n  probs = decoder_forward(y, enc)\n  l = logkldivergence(label[:, 2:end, :], probs[:, 1:end-1, :])\n  return l\nend\n\n#collect all the parameters\nps = params(embed, pe, encode_t1, encode_t2, decode_t1, decode_t2, linear)\nopt = ADAM(1e-4)\n\n#function for created batched data\nusing Transformers.Datasets: batched\n\n#flux function for update parameters\nusing Flux: gradient\nusing Flux.Optimise: update!\n\n#define training loop\nfunction train!()\n  @info \"start training\"\n  for i = 1:1000\n    data = batched([sample_data() for i = 1:32]) #create 32 random sample and batched\n\tx, y = preprocess.(data[1]), preprocess.(data[2])\n    x, y = vocab(x), vocab(y) #encode the data\n    x, y = todevice(x, y) #move to gpu\n    grad = gradient(()->loss(x, y), ps)\n    if i % 8 == 0\n        l = loss(x, y)\n    \tprintln(\"loss = $l\")\n    end\n    update!(opt, ps, grad)\n  end\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"train!()","category":"page"},{"location":"tutorial/#Test-our-model","page":"Tutorial","title":"Test our model","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"After training, we can try to test the model.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Flux: onecold\nfunction translate(x)\n    ix = todevice(vocab(preprocess(x)))\n    seq = [startsym]\n\n    enc = encoder_forward(ix)\n\n    len = length(ix)\n    for i = 1:2len\n        trg = todevice(vocab(seq))\n        dec = decoder_forward(trg, enc)\n        #move back to gpu due to argmax wrong result on CuArrays\n        ntok = onecold(collect(dec), labels)\n        push!(seq, ntok[end])\n        ntok[end] == endsym && break\n    end\n  seq[2:end-1]\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"translate(map(string, [5,5,6,6,1,2,3,4,7, 10]))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"10-element Vector{String}:\n \"5\"\n \"5\"\n \"6\"\n \"6\"\n \"1\"\n \"2\"\n \"3\"\n \"4\"\n \"7\"\n \"10\"\n","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The result looks good!","category":"page"},{"location":"bert/#Transformers.BidirectionalEncoder","page":"BERT","title":"Transformers.BidirectionalEncoder","text":"","category":"section"},{"location":"bert/","page":"BERT","title":"BERT","text":"Implementation of BERT model","category":"page"},{"location":"bert/#Get-Pretrain","page":"BERT","title":"Get Pretrain","text":"","category":"section"},{"location":"bert/","page":"BERT","title":"BERT","text":"One of the pleasant feature BERT give us is that it make using a large pre-trained model on a language task possible. To use a pretrained model with Transformers.jl, you can either:","category":"page"},{"location":"bert/","page":"BERT","title":"BERT","text":"Use the public released weight with pretrain\"\" as show in the pretrain section\nConvert the TensorFlow checkpoint file with BidirectionalEncoder.tfckpt2bson(need TensorFlow.jl installed). As long as the checkpoint is produced by the origin bert released by google, it should work without any issue. ","category":"page"},{"location":"bert/#Finetuning","page":"BERT","title":"Finetuning","text":"","category":"section"},{"location":"bert/","page":"BERT","title":"BERT","text":"After getting the pretrain weight in Julia. We are going to finetune the model on your dataset. The bert model is also a Flux layer, so  training bert is just like training other Flux model (i.e. all the usage should be compatibled with Flux's API)","category":"page"},{"location":"bert/#API-reference","page":"BERT","title":"API reference","text":"","category":"section"},{"location":"bert/","page":"BERT","title":"BERT","text":"Bert\nBidirectionalEncoder.WordPiece\nBidirectionalEncoder.BertTextEncoder\nBidirectionalEncoder.bert_cased_tokenizer\nBidirectionalEncoder.bert_uncased_tokenizer\nBidirectionalEncoder.tfckpt2bson\nBidirectionalEncoder.load_bert_pretrain\nmasklmloss\nbert_pretrain_task\nBidirectionalEncoder.recursive_readdir","category":"page"},{"location":"bert/#Transformers.BidirectionalEncoder.Bert","page":"BERT","title":"Transformers.BidirectionalEncoder.Bert","text":"Bert(size::Int, head::Int, ps::Int, layer::Int;\n    act = gelu, pdrop = 0.1, attn_pdrop = 0.1)\nBert(size::Int, head::Int, hs::Int, ps::Int, layer::Int;\n    act = gelu, pdrop = 0.1, attn_pdrop = 0.1)\n\nthe Bidirectional Encoder Representations from Transformer(BERT) model.\n\n(bert::Bert)(x, mask=nothing; all::Bool=false)\n\neval the bert layer on input x. If length mask is given (in shape (1, seqlen, batchsize)), mask the attention with getmask(mask, mask). Moreover, set all to true to get all  outputs of each transformer layer.\n\n\n\n\n\n","category":"type"},{"location":"bert/#Transformers.BidirectionalEncoder.BertTextEncoder","page":"BERT","title":"Transformers.BidirectionalEncoder.BertTextEncoder","text":"struct BertTextEncoder{T<:AbstractTokenizer,\n                       V<:AbstractVocabulary{String},\n                       P} <: AbstractTextEncoder\n  tokenizer::T\n  vocab::V\n  process::P\n  startsym::String\n  endsym::String\n  trunc::Union{Nothing, Int}\nend\n\nThe text encoder for Bert model. Taking a tokenizer, vocabulary, and a processing function, configured with  a start symbol, an end symbol, and a maximum length.\n\nBertTextEncoder(bert_tokenizer, wordpiece, process;\n                startsym = \"[CLS]\", endsym = \"[SEP]\", trunc = nothing)\n\nThere are two tokenizer supported (bert_cased_tokenizer and bert_uncased_tokenizer).  process can be omitted, then a predefined processing pipeline will be used.\n\nBertTextEncoder(f, bertenc::BertTextEncoder)\n\nTake a bert text encoder and create a new bert text encoder with same configuration except the processing function.  f is a function that take the encoder and return a new process function. This is useful for changing part of  the procssing function.\n\nExample\n\njulia> wordpiece = pretrain\"bert-cased_L-12_H-768_A-12:wordpiece\"\n[ Info: loading pretrain bert model: cased_L-12_H-768_A-12.tfbson wordpiece\nWordPiece(vocab_size=28996, unk=[UNK], max_char=200)\n\njulia> bertenc = BertTextEncoder(bert_cased_tokenizer, wordpiece; trunc=5)\nBertTextEncoder(\n├─ TextTokenizer(WordPieceTokenization(bert_cased_tokenizer, WordPiece(vocab_size=28996, unk=[UNK], max_char=200))),\n├─ vocab = Vocab{String, SizedArray}(size = 28996, unk = [UNK], unki = 101),\n├─ startsym = [CLS],\n├─ endsym = [SEP],\n├─ trunc = 5,\n└─ process = Pipelines:\n  ╰─ target[tok] := nestedcall(string_getvalue, source)\n  ╰─ target[tok] := with_firsthead_tail([CLS], [SEP])(target.tok)\n  ╰─ target[(tok, segment)] := segment_and_concat(target.tok)\n  ╰─ target[trunc_tok] := trunc_and_pad(5, [UNK])(target.tok)\n  ╰─ target[trunc_len] := nestedmaxlength(target.trunc_tok)\n  ╰─ target[mask] := getmask(target.tok, target.trunc_len)\n  ╰─ target[tok] := nested2batch(target.trunc_tok)\n  ╰─ target[segment] := (nested2batch ∘ trunc_and_pad(5, 1))(target.segment)\n  ╰─ target[input] := (NamedTuple{(:tok, :segment)} ∘ tuple)(target.tok, target.segment)\n  ╰─ target := (target.input, target.mask)\n)\n\n# take the first 3 pipeline and get the result\njulia> BertTextEncoder(bertenc) do enc\n           Pipelines(enc.process[1:3]) |> PipeGet{(:tok, :segment)}()\n       end\nBertTextEncoder(\n├─ TextTokenizer(WordPieceTokenization(bert_cased_tokenizer, WordPiece(vocab_size=28996, unk=[UNK], max_char=200))),\n├─ vocab = Vocab{String, SizedArray}(size = 28996, unk = [UNK], unki = 101),\n├─ startsym = [CLS],\n├─ endsym = [SEP],\n├─ trunc = 5,\n└─ process = Pipelines:\n  ╰─ target[tok] := nestedcall(string_getvalue, source)\n  ╰─ target[tok] := with_firsthead_tail([CLS], [SEP])(target.tok)\n  ╰─ target[(tok, segment)] := segment_and_concat(target.tok)\n  ╰─ target := (target.tok, target.segment)\n)\n\n\n\n\n\n\n","category":"type"},{"location":"bert/#Transformers.BidirectionalEncoder.bert_cased_tokenizer","page":"BERT","title":"Transformers.BidirectionalEncoder.bert_cased_tokenizer","text":"bert_cased_tokenizer(input)\n\ngoogle bert tokenizer which remain the case during tokenization. Recommended for multi-lingual data.\n\n\n\n\n\n","category":"function"},{"location":"bert/#Transformers.BidirectionalEncoder.bert_uncased_tokenizer","page":"BERT","title":"Transformers.BidirectionalEncoder.bert_uncased_tokenizer","text":"bert_uncased_tokenizer(input)\n\ngoogle bert tokenizer which do lower case on input before tokenization.\n\n\n\n\n\n","category":"function"},{"location":"bert/#Transformers.BidirectionalEncoder.tfckpt2bson","page":"BERT","title":"Transformers.BidirectionalEncoder.tfckpt2bson","text":"tfckpt2bson(path;\n            raw=false,\n            saveto=\"./\",\n            confname = \"bert_config.json\",\n            ckptname = \"bert_model.ckpt\",\n            vocabname = \"vocab.txt\")\n\nturn google released bert format into BSON file. Set raw to true to remain the origin data format in bson.\n\n\n\n\n\n","category":"function"},{"location":"bert/#Transformers.BidirectionalEncoder.load_bert_pretrain","page":"BERT","title":"Transformers.BidirectionalEncoder.load_bert_pretrain","text":"load_bert_pretrain(path, sym = :all)\n\nLoading bert data/model from pretrain bson data. use sym to determine which data to load. possible value: :all(default), bert_model, wordpiece, tokenizer.\n\n\n\n\n\n","category":"function"},{"location":"bert/#Transformers.BidirectionalEncoder.masklmloss","page":"BERT","title":"Transformers.BidirectionalEncoder.masklmloss","text":"masklmloss(embed::Embed{T}, transform,\n           t::AbstractArray{T, N}, posis::AbstractArray{Tuple{Int,Int}}, labels) where {T,N}\nmasklmloss(embed::Embed{T}, transform, output_bias,\n           t::AbstractArray{T, N}, posis::AbstractArray{Tuple{Int,Int}}, labels) where {T,N}\n\nhelper function for computing the maks language modeling loss.  Performance transform(x) .+ output_bias where x is the mask specified by  posis, then compute the similarity with embed.embedding and crossentropy between true labels.\n\n\n\n\n\n","category":"function"},{"location":"bert/#Transformers.BidirectionalEncoder.bert_pretrain_task","page":"BERT","title":"Transformers.BidirectionalEncoder.bert_pretrain_task","text":"bert_pretrain_task(datachn::Channel, wordpiece::WordPiece;\n                   buffer_size = 100,\n                   channel_size = 100\n                   wordpiece::WordPiece,\n                   sentences_pool = sentences;\n                   start_token = \"[CLS]\",\n                   sep_token = \"[SEP]\",\n                   mask_token = \"[MASK]\",\n                   mask_ratio = 0.15,\n                   real_token_ratio = 0.1,\n                   random_token_ratio = 0.1,\n                   whole_word_mask = false,\n                   next_sentence_ratio = 0.5,\n                   next_sentence = true,\n                   tokenizer = tokenize,\n                   istokenized = false,\n                   return_real_sentence = false)\n\nhelper function to generate bert mask language modeling and next sentence prediction data. datachn is a Channel with input documents line by line.\n\n\n\n\n\n","category":"function"},{"location":"bert/#Transformers.BidirectionalEncoder.recursive_readdir","page":"BERT","title":"Transformers.BidirectionalEncoder.recursive_readdir","text":"recursive_readdir(path::AbstractString=\"./\")\n\nrecursive read all file from a dir. return a list of filenames.\n\n\n\n\n\n","category":"function"}]
}
